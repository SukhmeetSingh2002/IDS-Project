{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/sukhmeet/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: imblearn in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sukhmeet/miniconda3/envs/tf/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import load_model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_TRAINING = \"../dataset/UNSW_NB15_training-set.csv\"\n",
    "FILE_PATH_TESTING = \"../dataset/UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "df_training =   pd.read_csv(FILE_PATH_TRAINING, index_col=0)\n",
    "df_testing =    pd.read_csv(FILE_PATH_TESTING, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((175341, 44), (82332, 44))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.shape, df_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dur                  0\n",
      "proto                0\n",
      "service              0\n",
      "state                0\n",
      "spkts                0\n",
      "dpkts                0\n",
      "sbytes               0\n",
      "dbytes               0\n",
      "rate                 0\n",
      "sttl                 0\n",
      "dttl                 0\n",
      "sload                0\n",
      "dload                0\n",
      "sloss                0\n",
      "dloss                0\n",
      "sinpkt               0\n",
      "dinpkt               0\n",
      "sjit                 0\n",
      "djit                 0\n",
      "swin                 0\n",
      "stcpb                0\n",
      "dtcpb                0\n",
      "dwin                 0\n",
      "tcprtt               0\n",
      "synack               0\n",
      "ackdat               0\n",
      "smean                0\n",
      "dmean                0\n",
      "trans_depth          0\n",
      "response_body_len    0\n",
      "ct_srv_src           0\n",
      "ct_state_ttl         0\n",
      "ct_dst_ltm           0\n",
      "ct_src_dport_ltm     0\n",
      "ct_dst_sport_ltm     0\n",
      "ct_dst_src_ltm       0\n",
      "is_ftp_login         0\n",
      "ct_ftp_cmd           0\n",
      "ct_flw_http_mthd     0\n",
      "ct_src_ltm           0\n",
      "ct_srv_dst           0\n",
      "is_sm_ips_ports      0\n",
      "attack_cat           0\n",
      "label                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create a boolean mask of missing values\n",
    "missing_values_mask = df_training.isnull()\n",
    "\n",
    "# count the number of missing values in each column\n",
    "missing_values_count = missing_values_mask.sum()\n",
    "\n",
    "# print the result\n",
    "print(missing_values_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proto\n",
      "tcp       79946\n",
      "udp       63283\n",
      "unas      12084\n",
      "arp        2859\n",
      "ospf       2595\n",
      "          ...  \n",
      "rdp          98\n",
      "netblt       98\n",
      "igmp         18\n",
      "icmp         15\n",
      "rtp           1\n",
      "Name: count, Length: 133, dtype: int64\n",
      "\n",
      "service\n",
      "-           94168\n",
      "dns         47294\n",
      "http        18724\n",
      "smtp         5058\n",
      "ftp-data     3995\n",
      "ftp          3428\n",
      "ssh          1302\n",
      "pop3         1105\n",
      "dhcp           94\n",
      "snmp           80\n",
      "ssl            56\n",
      "irc            25\n",
      "radius         12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "state\n",
      "INT    82275\n",
      "FIN    77825\n",
      "CON    13152\n",
      "REQ     1991\n",
      "RST       83\n",
      "ECO       12\n",
      "PAR        1\n",
      "URN        1\n",
      "no         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "attack_cat\n",
      "Normal            56000\n",
      "Generic           40000\n",
      "Exploits          33393\n",
      "Fuzzers           18184\n",
      "DoS               12264\n",
      "Reconnaissance    10491\n",
      "Analysis           2000\n",
      "Backdoor           1746\n",
      "Shellcode          1133\n",
      "Worms               130\n",
      "Name: count, dtype: int64\n",
      "\n",
      "proto\n",
      "tcp           43095\n",
      "udp           29418\n",
      "unas           3515\n",
      "arp             987\n",
      "ospf            676\n",
      "              ...  \n",
      "br-sat-mon       32\n",
      "pvp              32\n",
      "wsn              32\n",
      "ib               31\n",
      "igmp             30\n",
      "Name: count, Length: 131, dtype: int64\n",
      "\n",
      "service\n",
      "-           47153\n",
      "dns         21367\n",
      "http         8287\n",
      "smtp         1851\n",
      "ftp          1552\n",
      "ftp-data     1396\n",
      "pop3          423\n",
      "ssh           204\n",
      "ssl            30\n",
      "snmp           29\n",
      "dhcp           26\n",
      "radius          9\n",
      "irc             5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "state\n",
      "FIN    39339\n",
      "INT    34163\n",
      "CON     6982\n",
      "REQ     1842\n",
      "ACC        4\n",
      "RST        1\n",
      "CLO        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "attack_cat\n",
      "Normal            37000\n",
      "Generic           18871\n",
      "Exploits          11132\n",
      "Fuzzers            6062\n",
      "DoS                4089\n",
      "Reconnaissance     3496\n",
      "Analysis            677\n",
      "Backdoor            583\n",
      "Shellcode           378\n",
      "Worms                44\n",
      "Name: count, dtype: int64\n",
      "\n",
      "state ACC\n",
      "state CLO\n"
     ]
    }
   ],
   "source": [
    "# value counts\n",
    "unique_values = {}\n",
    "for col in df_training.columns:\n",
    "    if df_training[col].dtype == object:\n",
    "        print(df_training[col].value_counts())\n",
    "        print()\n",
    "        unique_values[col] = df_training[col].unique()\n",
    "\n",
    "unique_values_test = {}\n",
    "for col in df_testing.columns:\n",
    "    if df_testing[col].dtype == object:\n",
    "        print(df_testing[col].value_counts())\n",
    "        print()\n",
    "        unique_values_test[col] = df_testing[col].unique()\n",
    "\n",
    "# swap\n",
    "# unique_values, unique_values_test = unique_values_test, unique_values\n",
    "# vlaues which are in test but not in train\n",
    "for col in unique_values_test.keys():\n",
    "    for val in unique_values_test[col]:\n",
    "        if val not in unique_values[col]:\n",
    "            print(col, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()\n",
    "\n",
    "# print the columns\n",
    "for idx, col in enumerate(df_training.columns):\n",
    "    print(idx, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_training.columns, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_training.columns:\n",
    "    if 50 >=df_training[col].nunique() :\n",
    "        print(col, df_training[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.describe()\n",
    "# before log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.describe()\n",
    "# After log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal_distribution(df):\n",
    "    for column in df.columns:\n",
    "        sns.displot(df[column])\n",
    "        plt.show()\n",
    "        \n",
    "plot_normal_distribution(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['proto','state','service','attack_cat']\n",
    "df_training.drop(columns_to_remove, axis=1, inplace=True)\n",
    "df_testing.drop(columns_to_remove, axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['proto','state','service','label']\n",
    "df_training.drop(columns_to_remove, axis=1, inplace=True)\n",
    "df_testing.drop(columns_to_remove, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_training.columns:\n",
    "    if column !='label' and df_training[column].nunique() > 50:\n",
    "        df_training[column] = np.log(df_training[column]+1)\n",
    "        df_testing[column] = np.log(df_testing[column]+1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "for column in df_training.columns:\n",
    "    if column != 'label' and column != 'attack_cat':\n",
    "        standard_scaler.fit(df_training[column].values.reshape(-1,1))\n",
    "        df_training[column] = standard_scaler.transform(df_training[column].values.reshape(-1,1))\n",
    "        df_testing[column] = standard_scaler.transform(df_testing[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after standardization\n",
    "df_training.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_training.drop('label', axis=1)\n",
    "y_train = df_training['label']\n",
    "\n",
    "X_test = df_testing.drop('label', axis=1)\n",
    "y_test = df_testing['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_training.drop('attack_cat', axis=1)\n",
    "y_train = df_training['attack_cat']\n",
    "\n",
    "X_test = df_testing.drop('attack_cat', axis=1)\n",
    "y_test = df_testing['attack_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode attack_cat using label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_train = labelencoder_y.fit_transform(y_train)\n",
    "y_test = labelencoder_y.transform(y_test)\n",
    "\n",
    "\n",
    "# y_train = pd.get_dummies(y_train)\n",
    "# y_test = pd.get_dummies(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f={}\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] not in f:\n",
    "        f[y_train[i]]=1\n",
    "    else:\n",
    "        f[y_train[i]]+=1\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle class imbalance\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = dict(enumerate((max(class_counts) / class_counts).astype(float)))\n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Create a RandomOverSampler object\n",
    "oversampler = RandomOverSampler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train_oversampled, y_train_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape,X_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit a Random Forest model using class weights\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=50, random_state=42, verbose=1, criterion='entropy',warm_start=False)\n",
    "rf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': feature_importances}).sort_values('importance', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.barplot(x = 'importance', y = 'feature', data = feature_importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with importance greater than 0.01\n",
    "selected_features = feature_importances[feature_importances['importance'] > 0.05]['feature'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of features selected vs total number of features\n",
    "len(selected_features), X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training and testing data with selected features\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "X_val = X_val[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape,X_val.shape,y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_ann():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(50, input_dim=X_train.shape[1], activation='relu'))\n",
    "  model.add(Dense(25, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def get_model_ann_multiclass():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(50, input_dim=X_train.shape[1], activation='relu'))\n",
    "  model.add(Dense(25, activation='relu'))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# model_ann = get_model_ann()  \n",
    "model_ann = get_model_ann_multiclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ann = model_ann.fit(X_train, y_train, epochs=50, batch_size=256, verbose=1, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_ann = load_model('models/saved_models/model_ann_20231103-112820_0.887.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_ann = model_ann.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_prediction_ann = np.where(y_prediction_ann > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScore_ann = accuracy_score(y_test, y_prediction_ann)\n",
    "print(\"ANN Accuracy: \", accuracyScore_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_ann.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %f' % (loss*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi ClassPredicition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction_ann_multiclass = np.argmax(y_prediction_ann, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "accuracyScore_ann = accuracy_score(y_test, y_prediction_ann_multiclass)\n",
    "print(\"ANN Accuracy with multiclass: \", accuracyScore_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graph of probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities(y_prediction_array, true_label,predicted_label=None):\n",
    "  if predicted_label is None:\n",
    "    predicted_label = np.argmax(y_prediction_array)\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), y_prediction_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "for i in range(56, 60):\n",
    "  plot_probabilities(y_prediction_ann[i], y_test[i], y_prediction_ann_multiclass[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_prediction, title):\n",
    "    cm = confusion_matrix(y_test, y_prediction)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "plot_confusion_matrix(y_test, y_prediction_ann_multiclass, \"ANN Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with time and accuracy\n",
    "import datetime\n",
    "model_ann.save('model_ann_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+str(accuracyScore_ann)[0:5]+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_prediction_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## give confusion matrix\n",
    "cm_ann = confusion_matrix(y_test, y_prediction_ann)\n",
    "##  plot it\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('ANN Confusion Matrix')\n",
    "sns.heatmap(cm_ann, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roc Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_ann)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=50, random_state=42, verbose=1, criterion='entropy',warm_start=True)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for xgboost\n",
    "xgb = XGBClassifier(n_estimators=100, n_jobs=50, random_state=42, verbose=1, criterion='entropy',warm_start=True)\n",
    "xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "y_pred = xgb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(25, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(10))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model_lstm = get_model_lstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for LSTM\n",
    "X_train = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "X_val = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=100, batch_size=1024, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "y_prediction_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_prediction_lstm = np.where(y_prediction_lstm > threshold, 1, 0)\n",
    "\n",
    "accuracyScore_lstm = accuracy_score(y_test, y_prediction_lstm)\n",
    "\n",
    "print(\"LSTM Accuracy: \", accuracyScore_lstm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with time and accuracy\n",
    "import datetime\n",
    "model_lstm.save('model_lstm_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_'+str(accuracyScore_lstm)[0:5]+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "loss_lstm, accuracy_lstm = model_lstm.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy_lstm*100))\n",
    "print('Loss: %f' % (loss_lstm*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss during training\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Loss')\n",
    "plt.plot(history_lstm.history['loss'], label='train')\n",
    "plt.plot(history_lstm.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_prediction, title):\n",
    "    cm = confusion_matrix(y_test, y_prediction)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_prediction_lstm, \"LSTM Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
